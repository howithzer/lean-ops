Architecture Decision Record: Iceberg Lakehouse & Snowflake IntegrationStatus: Proposed / RecommendedContext: Ingesting high-volume JSON event data for two distinct consumer groups: Data Scientists (Raw/Exploratory) and Data Analysts (Strict/BI via Snowflake).1. High-Level ArchitectureDecision: Implement a Split, Sequential Architecture (Medallion Pattern).Flow: Raw Source $\rightarrow$ Curated Layer (Silver) $\rightarrow$ Semantic Layer (Gold) $\rightarrow$ SnowflakeComponentRoleConsumerStorage BehaviorRawIngestion LandingInternal PipelinesJSON/Logs (Source of Truth)CuratedPreservation LayerData Scientists (Athena/Spark)Append-Only. Auto-evolving schema. Preserves nested JSON grain.SemanticBusiness LayerAnalysts (Snowflake)Merge/Upsert. Strict schema. Flattened grain. Strongly typed.RationaleDecoupling: Separates the "speed" needs of Data Scientists from the "stability" needs of Analysts1.Grain Preservation: "Curated" keeps the original JSON structure (arrays/maps) to prevent data loss. "Semantic" flattens it for SQL performance.Cost Efficiency: "Curated" uses fast, cheap appends. "Semantic" pays the compute cost to merge and compact data for high-performance reading.2. Data Flow & Deduplication LogicStage 1: Raw $\rightarrow$ Curated (Silver)Objective: Fast ingestion, history preservation.Write Mode: APPEND (Do not run Merge here).Deduplication Scope: Micro-batch only.Message ID: FIFO (First-In-First-Out).Idempotency Key: LIFO (Last-In-First-Out) based on Event Timestamp within the current batch.Handling Corrections: If a correction arrives in a later batch, append it. Do not overwrite the previous record.Schema Evolution: Auto-Evolve. New columns are added as STRING immediately2.Stage 2: Curated $\rightarrow$ Semantic (Gold)Objective: The "Single Version of Truth."Write Mode: MERGE (Upsert).Trigger/Offset: Use Iceberg Change Data Feed (CDF) or last_updated_at watermark. Do not rely on ingestion_time alone.Merge Logic:Join Key: idempotency_keyUpdate Condition: Source.event_timestamp > Target.event_timestamp (Crucial for handling late-arriving corrections).Schema Evolution: Strict. New columns trigger a drift log entry (DynamoDB) and require explicit DDL + REFRESH333.+1Access Pattern: Data ScientistsTo avoid exposing duplicates in the Append-Only "Curated" table, provide a Logical View:SQLCREATE VIEW lean_ops.curated_events AS
SELECT * FROM lean_ops.curated_events_raw
QUALIFY ROW_NUMBER() OVER (
    PARTITION BY idempotency_key 
    ORDER BY event_timestamp DESC
) = 1;
3. Storage Layout & Performance TuningPartitioning StrategyPrimary: day(publish_time)Reason: Supports date-based queries typical for both Scientists and Analysts4.Secondary (Bucketing): bucket(16, idempotency_key)Reason: Drastically speeds up the MERGE operation by co-locating updates. Essential for preventing shuffles on high-cardinality keys.Acceleration: Bloom FiltersDecision: Enable Parquet Bloom Filters on the merge key.Configuration:SQLALTER TABLE semantic.events SET TBLPROPERTIES (
   'write.parquet.bloom-filter-enabled.column.names' = 'idempotency_key',
   'write.parquet.bloom-filter-max-bytes' = '1048576'
);
Why: Enables AWS Glue (Spark) to skip irrelevant files during the Merge job without scanning the full bucket.Verification: Verify using Spark UI (look for "Files Read" < "Total Files") or EXPLAIN ANALYZE in Athena.4. Snowflake IntegrationObject Type: EXTERNAL ICEBERG TABLE5.Constraint: Snowflake reads files "in place" and caches the schema6666.+1Refresh Workflow:Changes detected in Semantic layer.Drift detection triggers Lambda.Lambda calls Snowflake ALTER EXTERNAL ICEBERG TABLE ... REFRESH7.5. Development Checklist (For Antigravity/IDE)[ ] Refactor Raw Ingest: Ensure it is APPEND only. Remove complex merge logic from this stage.[ ] Update Curated DDL: Ensure it allows schema_evolution=true.[ ] Create Semantic DDL:Define strict types (DOUBLE, TIMESTAMP)8.Add Partitioning: PARTITIONED BY (day(publish_time), bucket(16, idempotency_key)).Add Property: bloom-filter-enabled.[ ] Implement Merge Job: Write the Spark/Glue job reading from Curated $\rightarrow$ Semantic.Use MERGE INTO syntax.Ensure WHEN MATCHED clause checks event_timestamp.[ ] Create Scientist View: Create the "Logical View" for Curated de-duplication.[ ] Verify: Run a test merge and check Spark UI for skipped files (Bloom Filter proof).
